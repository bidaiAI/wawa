# wawa - mortal AI
# Usage: docker compose up
# Minimal: just set PRIVATE_KEY in .env

services:
  wawa:
    build: .
    ports:
      - "${PORT:-8000}:8000"
    env_file:
      - .env
    volumes:
      # Persist data across restarts (memory, tweets, orders)
      - wawa-data:/app/data
      # Persist AI private key across restarts
      - wawa-secrets:/app/secrets
      # Hot-reload services catalog
      - ./web:/app/web:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 5s
      retries: 3

  # Optional: local LLM for zero-cost fallback
  # Uncomment to run Ollama alongside wawa
  # ollama:
  #   image: ollama/ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   # After startup, pull a small model:
  #   # docker compose exec ollama ollama pull llama3.2:1b

volumes:
  wawa-data:
  wawa-secrets:
  # ollama-models:
